\documentclass[12pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage[legalpaper, portrait, margin=0.8in]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}

\title{CDA5106 - Advanced Computer Architecture \\ Final Exam Review}
\author{Kobee Raveendran}
\date{}

\begin{document}
    \maketitle

    \section{Module 1: High-Performance Microprocessor Architecture}

    \subsection{Module 1.2: Power Wall and Dennard Scaling}

    \subsubsection{Notes}

    \begin{itemize}
        \item energy: ability of a physical system to do work on other physical systems (unit: joule)
        \item power: rate at which energy is transformed (unit: watt; 1 watt = 1 joule delivered per second)
        
        \begin{itemize}
            \item power = $V \cdot I$ (V = voltage, I = current)
        \end{itemize}
        
        \item for capacitors:
        
        \begin{itemize}
            \item energy stored = $0.5 \cdot C \cdot V^2$ (C = capacitance, V = voltage)
            \item if a capacitor is drained at a frequency of $f$ per second: 
            power = $\frac{energy}{second} = 2 \cdot 0.5 CV^2 = CV^2$
        \end{itemize}

        \item Power wall problem
        \begin{itemize}
            \item $P_{dyn} = ACV^2f$
            \item A: fraction of gates actively switching
            \item C: total capacitance of all gates
            \item V: supply voltage
            \item f: frequency of switching
        \end{itemize}

        \item Power wall fundamentals
        \begin{itemize}
            \item max frequency vs. threshold voltage:
            \item $f_{max} = c \cdot \frac{(V - V_{thd})^{1.3}}{V}$
        \end{itemize}

        \item \textbf{Dennard Scaling Example (old)}
        \begin{itemize}
            \item if gate length (transistor size) scales by $S = 0.7$ (both length and width), then:
            \item capacitance scales by $S = 0.7$
            \item original area scales by $S^2 = 0.5$
            \item number of transistors scales by $\frac{1}{S^2} \approx 2$
            \item supply voltage ($V$) scales by $S = 0.7$
            \item frequency ($f$) scales by $\frac{1}{S} = 1.4$
            \item then, \textbf{dynamic power} $P_{dyn} = ACV^2f$
            \item and \textbf{new dynamic power} $P_{dyn}' = A'C'V'^2f'$
            \item $P_{dyn}' = (2A)(0.7C)(0.7V)^2(1.4f) \approx 1 \cdot ACV^2f = P_{dyn}$
        \end{itemize}

        \item \textbf{Post Dennard Scaling example (new)}
        \begin{itemize}
            \item capacitance scales by $S = 0.7$
            \item number of transistors scales by $\frac{1}{S^2} = 2$
            \item supply voltage ($V$) cannot scale without also scaling threshold voltage ($V_{thd}$), and doing that 
            increases static power exponentially
            \item frequency ($f$) scales by $\frac{1}{S} = 1.4$
            \item result: dynamic power doubles every generation
            \item $P_{dyn} = ACV^2f$
            \item $P_{dyn}' = A'C'V'^2f' = (2A)(0.7C)(1 \cdot V)^2(1.4f) \approx 2 \cdot P_{dyn}$
        \end{itemize}

        \subsubsection{Exercises}

			\begin{enumerate}
				\item Suppose that instead of progressing at a ratio of 0.7, Mooreâ€™s law slows down and transistor gate length scales at a ratio of 0.8 instead. Find the dynamic power consumption under \textit{unlimited} and \textit{limited} scaling for the next process generation.

				\begin{itemize}
					\item Unlimited/old scaling rule
					\begin{itemize}
						\item gate length scales by $S = 0.8$
						\item capacitance scales by $S = 0.8$
						\item original area scales by $S^2 = 0.64$
						\item num transistors thus scales by $\frac{1}{S^2} = 1.56$
						\item supply voltage scales by $S = 0.8$
						\item frequency scales by $\frac{1}{S} = 1.25$
						\item dynamic power stays constant:
					
						$P_{dyn} = (1.56A)(0.8C)(0.8V)^2(1.25f)$
					\end{itemize}
					
					\item leakage-limited/new scaling
					\begin{itemize}
						\item capacitance scales by $S = 0.8$
						\item num transistors scales by $\frac{1}{S^2} = 1.56$
						\item supply voltage does not scale without scaling threshold voltage too, which increases static power exponentially
						\item frequency scales by $\frac{1}{S} 1.25$
						\item dynamic power consumption increases:

						$P_{dyn}' = (1.56A)(0.8C)(V)^2(1.25f) = 1.56 \cdot P_{dyn}$
					\end{itemize}
				\end{itemize}

				\item With limited voltage scaling, suppose that we want to keep the dynamic power consumption constant in the next generation by keeping frequency constant and reduce die area. How much should we reduce die area to achieve that?
				\begin{itemize}
					\item gate length scales by $S = 0.7$
					\item capacitance scales by $S = 0.7$
					\item original area scales by $S^2 = 0.5$
					\item supply voltage and frequency are constant
					\item dynamic power consumption must stay constant: $P_{dyn}' = P_{dyn}$

					$ACV^2f = A'(0.7C)V^2f \longrightarrow A = 0.7A'$

					\item number of transistors in the next generation: $A' = 1.4A$ (instead of 2A like before; i.e. 70\% of 2A)
					\item thus die area shrinks by 30\%
				\end{itemize}

				\item Describe the difference between energy and power.

				Power is the rate of energy consumption.

				\item Describe the impact of threshold voltage choice on static and dynamic power consumption as transistors are scaled down.

				If threshold voltage is lowered, dynamic power decreases (nearly linearly) but static power increases exponentially.

				\item How has processor design adapted to the power wall problem?

				Stalling frequency growth, multicore, and sophisticated power management (clock gating, voltage and frequency scaling, power gating).
			\end{enumerate}

    \end{itemize}

	\subsubsection{Overview of ILP Techniques}

	Caches example

	\begin{itemize}
		\item processor with 1-ns clock
		\item 64KB cache memory with 2-ns read time, 95\% hitrate
		\item 512MB main memory with 150-ns read time
		\item What is the average access time (AAT) in this memory system?
	\end{itemize}

	Answer:
	\begin{itemize}
		\item hits: $95 \cdot 2$ ns, misses: $5 \cdot (2 + 150)$ ns
		\item total = hit time + miss time = $190 + (10 + 750) = 950 ns$
		\item AAT = $\frac{total}{100} = 9.5ns$
	\end{itemize}

	\section{Module 2: Performance, Cost, and Reliability of Microprocessors}

	\subsection{Performance Evaluation 1}

	\subsubsection{Amdahl's Law}

	\begin{itemize}
		\item performance improvement (``speedup'') is limited by the part you can't improve
		\item (s) $Speedup_{enhanced} =$ best case speedup from gizmo alone
		\item (f) $Fraction_{enhanced} =$ fraction of task that gizmo can enhance
		\item $s_{overall} = \frac{1}{(1 - f) + \frac{f}{s}}$
	\end{itemize}

	Example:

	\begin{itemize}
		\item jet plane wing simulation, where 1 run takes 1 week on your computer
		\item your program is 80\% parallelizable
		\item new supercomputer has 100,000 processors
		\item $s = 100,000$
		\item $f = 0.8$
		\item overall speedup: $s_{overall} = \frac{1}{(1 - f) + \frac{f}{s}} = \frac{1}{(1 - 0.8) + \frac{0.8}{100000}} \approx \frac{1}{0.2} = 5$
		\item only about 5 times faster (33 hours instead of 1 week), but not worth the high price tag (using a cheaper computer with only 100 processors instead yields a 4.8X speedup!)
	\end{itemize}

	More examples:

	Ex 1:

	\begin{itemize}
		\item $f = 0.95$
		\item $s = 1.10$
		\item $s_{overall} = \frac{1}{(1-0.95) + \frac{0.95}{1.10}} = 1.094 \approx 1.10$
	\end{itemize}

	Ex 2:

	\begin{itemize}
		\item $f = 0.05$
		\item $s \rightarrow \infty$
		\item $s_{overall} = 1.053$
	\end{itemize}

	\subsubsection{Run Time}

	\begin{itemize}
		\item CPU time = clock cycle count $\times$ cycle time
		\item cycles per instruction (CPI) = $\frac{\texttt{clock cycle count}}{\texttt{instruction count}}$
		\item CPU time = IC $\times$ CPI $\times$ CT
	\end{itemize}

	\subsection{Performance Evaluation 2}

	Determine speedup by comparing program times with respect to a reference machine.

	

	\begin{itemize}
		\item arithmetic mean (which one should we trust?):
		
		\begin{tabular}[h!]{|c|c|c|c|} \hline
					   & Computer A & Computer B & B vs. A   \\ \hline
			Program P1 & 2X faster  & 4X faster  & 2X faster \\ \hline
			Program P2 & 5X faster  & 15X faster & 3X faster \\ \hline
			Average	   & 3.5X		& 9.5X		 &			 \\ \hline
		\end{tabular}

		Speedups:

		\begin{itemize}
			\item method 1: program-wise $\longrightarrow \frac{2 + 3}{2} = 2.5$X faster
			\item method 2: machine-wise $\longrightarrow \frac{9.5}{3.5} = 2.71$X faster
		\end{itemize}

		\item geometric mean (consistent):
		
		$\displaystyle gmean = \sqrt[n]{\prod_{i = 1}^n} = exp(\frac{\frac{1}{n} \sum_{i=1}^n \ln (x_i)}{n})$

		\begin{tabular}[h!]{|c|c|c|c|} \hline
					   & Computer A 	& Computer B 	& B vs. A   	\\ \hline
			Program P1 & 2X faster  	& 4X faster  	& 2X faster 	\\ \hline
			Program P2 & 5X faster  	& 15X faster 	& 3X faster 	\\ \hline
			Average	   & $\sqrt{10}$	& $\sqrt{60}$	& $\sqrt{6}$	\\ \hline
		\end{tabular}

		Speedups:

		\begin{itemize}
			\item method 1: program-wise $\longrightarrow$ B is $\sqrt{2 \cdot 3} = \sqrt{6}$X faster
			\item method 2: machine-wise $\longrightarrow$ B is $\sqrt{60} \cdot \sqrt{10} = \sqrt{6}$X faster
		\end{itemize}

		\item (also important): geometric standard deviation
		
		$\displaystyle gstdev = exp(\sqrt{\frac{\prod_{i=1}^n (\ln x_i - \ln gmean)^2}{n}})$

		in plain English: for each ``component'' speedup vs. ref machine, take its natural log and subtract the natural 
		log of the gmean from that. Square it and multiply all of these together, then divide by n. Finally take the 
		square root of this, then take $e$ to the power of the result.
	\end{itemize}

	\subsubsection{Exercises}

	Given the following table of speedups for machines A and B relative to a reference machine:

	\begin{tabular}[h!]{|c|c|c|c|} \hline
		Prog	& X (secs)	& A (secs)	& B	(secs)	\\ \hline
		App 1	& 30		& 15		& 10		\\ \hline
		App 2	& 20		& 15		& 10		\\ \hline
		App 3	& 40		& 20		& 30		\\ \hline
		App 4	& 15		& 20		& 15		\\ \hline
	\end{tabular}

	Compute the following (see post-computation table below to find them all):

	\begin{itemize}
		\item geometric speedup of machine A vs. base machine X
		
		from the table, we find that A has a 1.41X speedup over X

		\item geometric speedup of machine B vs. base machine X
		
		from the table, we find that B has a 1.68X speedup over X

		\item geometric speedup of machine B vs. machine A
		
		from the table, we find that B has a 1.19X speedup over A

		\item geometric standard deviation of the speedup of machine A over machine X
		
		$gstd = exp(\sqrt{\frac{1}{4} \cdot \ln^2(\frac{2}{1.41}) \ln^2(\frac{1.33}{1.41}) \ln^2(\frac{2}{1.41}) \ln^2(\frac{0.75}{1.41})})$

		$gstd = 1.002255... \approx 1$

	\end{itemize}

	\begin{tabular}[h!]{|c|c|c|c|} \hline
		Prog		& A vs. X	& B vs. X	& B vs. A	\\ \hline
		App 1		& 2X		& 3X		& 1.5X		\\ \hline
		App 2		& 1.33X		& 2X		& 1.5X		\\ \hline
		App 3		& 2X		& 1.33X		& 0.67X		\\ \hline
		App 4		& 0.75X		& 1X		& 1.33X		\\ \hline
		\bf{Product}& 4X		& 8X		& 2X		\\ \hline
		\bf{gmean}	& 1.41X		& 1.68X		& \bf{1.19X}\\ \hline
	\end{tabular}

	\subsection{Cost and Reliability}

	\subsubsection{Failure Rates ($\lambda$)}

	\begin{itemize}
		\item $\lambda$ = the number of failures that occur per unit time in a component/system
		\item FIT (failure in time) = number of failures in $10^9$ hours
		\item example: 10,000 microprocessor chips used for 1,000 hours, and 8 of them fail. Failure rate is thus 
		$\frac{8}{10,000 \cdot 1,000} = 8 \cdot 10^{-7}$ (failures per hour per chip) $\cdot 10^9$ hours = 800 FITs
	\end{itemize}

	\subsubsection{Reliability Metrics}

	\begin{itemize}
		\item $R(t)$ = probability that the system still works correctly at time $t$
		\item $W_N(t)$ = number of items (of the same kind) that would still be working at time $t$
		\item if $\lambda$ is constant, then $R(t) = e^{-\lambda t}$
		\item Mean Time Between Failure (MTBF) = $\frac{1}{\lambda}$
	\end{itemize}

	\subsubsection{System Reliability}

	Assume that:

	\begin{itemize}
		\item $M$ components are in the system with failure rates $\lambda_1, \lambda_2, ..., \lambda_m$
		\item for the system to work properly, all components must also work properly
		\item a component's reliability is independent of any other component's reliability
		\item then, system failure rate = sum of component's failure rates
		\item $R_{sys}(t) = R_1(t) \cdot R_2(t) \cdot ... \cdot R_m(t) = e^{-\lambda_1 t \cdot ... \cdot -\lambda_mt}$
		$= e^{-(\lambda_1 + \lambda_2 + ... + \lambda_m)t} = e^{-\lambda_{sys}t}$
	\end{itemize}

	Other metrics:

	\begin{itemize}
		\item Mean Time To Repair (MTTR): mean time to repair/recover from a fault
		\item Mean Time Between Failure (MTBF): mean time between 2 consecutive failures
		\item if each failure is repaired, then MTBF = MTTF + MTTR
		\item usually, MTTF $\gg$ MTTR, so MTBF and MTTF are often interchangeable
	\end{itemize}

	\subsubsection{Examples}

	Assume a disk subsystem with:

	\begin{itemize}
		\item 10 disks each rated at $10^6$-hour MTTF
		\item 1 SCSI controller rated at $5 \cdot 10^5$-hour MTTF
		\item 1 power supply rated at $2 \cdot 10^5$-hour MTTF
		\item 1 fan rated at $2 \cdot 10^5$-hour MTTF
		\item 1 SCSI cable rated at $10^6$-hour MTTF
	\end{itemize}

	Find the failure rate of the entire disk subsystem.

	$R_{sys}(t) = 10 \cdot \frac{1}{10^6} + \frac{1}{5 \cdot 10^5} + \frac{2}{2 \cdot 10^5} + \frac{1}{10^6}$
	$			= \frac{10 + 2 + 5 + 5 + 1}{10^6} = \frac{23}{10^6} = \frac{23,000}{10^9} = 23,000$ FIT

	Thus, MTTF = $\frac{1}{\lambda_{sys}} = \frac{1}{23,000} \cdot 10^9 \approx 43,500$ hours

	\section{Instruction Set Design}

	\subsection{Instruction Set Architecture 1}

	\subsubsection{Styles of ISAs}

	\begin{itemize}
		\item Stack:
		\begin{itemize}
			\item \texttt{push <addr>, pop <addr>} (or ALU instructions)
			\item ALU: pop two entries, perform ALU operation, push result onto stack
			\item compact instruction format
			\begin{itemize}
				\item all calculation operations take 0 operands
				\item flexible; used for compiling Java bytecode (not dependent on registers in architecture [but all 
				have stacks])
			\end{itemize}
		\end{itemize}

		\item Accumulator:
		\begin{itemize}
			\item \texttt{load/store/ALU <addr>} (result affects accumulator register)
			\item also very compact (all operations take 1 operand, the other is implicitly the accumulator register)
			\item less dependence on memory than stack-based
		\end{itemize}

		\item Register-memory:
		\begin{itemize}
			\item \texttt{load/store/ALU <reg>, <reg/addr>}
			\begin{itemize}
				\item at most one operand can be a memory address
				\item leftmost register is the destination (if applicable)
			\end{itemize}
		\end{itemize}

		\item Load-Store:
		\begin{itemize}
			\item \texttt{load/store <reg>, <addr>}
			\item ex: \texttt{ALU <reg1>, <reg2>, <reg3>}: reg1 is destination, other 2 are source registers
		\end{itemize}
	\end{itemize}

	Example for adding two numbers:

	\begin{tabular}[ht!]{|c|c|c|c|} \hline
		Stack	& Accumulator	& Register-memory	& Load-Store	\\ \hline
		push A	& load A		& load R1 A			& load R1 A		\\ \hline
		push B	& add B			& add R1 B			& load R2 B		\\ \hline
		add		& store C		& store C R1		& add R3 R1 R2	\\ \hline
		pop C	& 				& 					& store C R3	\\ \hline
	\end{tabular}

	Pros/cons:

	\begin{itemize}
		\item load-store
		\begin{itemize}
			\item (+) fixed length instructions possible (allows for easy fetch/decode)
			\item (+) simpler hardware: efficient pipeline and potentially lower cycle time
			\item (-) higher instruction count
			\item (-) fixed-length instructions can be wasteful (more bits than needed for some instructions)
		\end{itemize}

		\item register-memory
		\begin{itemize}
			\item (+) no need for extra loads
			\item (+) better usage of bits (these pros lead to better code density)
			\item (-) destroys source operand(s) (i.e. \texttt{add R1 R2})
			\item (-) may impact cycles per instruction
		\end{itemize}

		\item memory-memory
		\begin{itemize}
			\item (+) most compact (code density)
			\item (-) high memory traffic (thus bottlenecked by memory)
		\end{itemize}
	\end{itemize}

	\subsection{Instruction Set Architecture 2}

	\subsubsection{RISC and Common Addressing Modes}

	\begin{itemize}
		\item register
		\begin{itemize}
			\item \texttt{add R4 R3 // R4 = R4 + R3}
			\item used when value is in a register
		\end{itemize}

		\item immediate
		\begin{itemize}
			\item \texttt{add R4 \#3 // R4 = R4 + 3}
			\item used for small constants (which occur frequently)
		\end{itemize}

		\item displacement
		\begin{itemize}
			\item \texttt{add R4 100(R1) // R4 = R4 + MEM[100 + R1]}
			\item accesses the frame (arguments, local variables)
			\item access the global data segment
			\item accesses the fields of a data struct
		\end{itemize}

		\item register deferred/register indirect
		\begin{itemize}
			\item \texttt{add R3 (R1) // R3 = R3 + MEM[R1]}
			\item accesses using a computed memory address
		\end{itemize}

		\item indexed
		\begin{itemize}
			\item \texttt{add R3 (R1 + R2) // R3 = R3 + MEM[R1 + R2]}
			\item array accesses: R1 = base, R2 = index
		\end{itemize}

		\item direct/absolute
		\begin{itemize}
			\item \texttt{add R1 (1001) // R1 = R1 + MEM[1001]}
			\item accessing global (``static'') data
		\end{itemize}

		\item memory deferred/memory indirect
		\begin{itemize}
			\item \texttt{add R1 @(R3) // R1 = R1 + MEM[MEM[R3]]}
			\item pointer dereferencing: \texttt{x = *p;} (if p is not register-allocated)
		\end{itemize}

		\item autoincrement/postdecrement
		\begin{itemize}
			\item \texttt{add R1 (R2)+ // R1 = R1 + MEM[R2]; R2 = R2 + \textit{d}} (\textit{d} is the size of the operation)
			\item looping through arrays, stack pop
		\end{itemize}

		\item autodecrement/predecrement
		\begin{itemize}
			\item \texttt{add R1 -(R2) // R1 = R1 + MEM[R2]; R2 = R2 - \textit{d}} (\textit{d} is the size of the operation)
			\item same uses as autoincrement, stack push
		\end{itemize}

		\item scaled
		\begin{itemize}
			\item \texttt{add R1 100(R2)[R3] // R1 = R1 + MEM[100 + R2 + R3 * \textit{d}]}
			\item array accesses for non-byte-sized elements
		\end{itemize}
	\end{itemize}

	\subsection{Instruction Set Architecture 3}

	\subsubsection{Condition Codes (for branch instructions)}

	\begin{itemize}
		\item Z
		\begin{itemize}
			\item zero flag
			\item indicates the result of an arithmetic/logical expression is zero
		\end{itemize}

		\item C
		\begin{itemize}
			\item carry flag
			\item indicates that an operation has a carry out. Enables numbers larger than a single word to be added/subtracted
		\end{itemize}

		\item S/N
		\begin{itemize}
			\item sign/negative flag
			\item indicates the result of an operation is negative
		\end{itemize}

		\item V/O/W
		\begin{itemize}
			\item overflow flag
			\item indicates the result of an operation is too large to fit in a register (using 2's complement representation)
		\end{itemize}
	\end{itemize}

	\subsubsection{Instruction Encoding Tradeoffs}

	\begin{itemize}
		\item variable width
		\begin{itemize}
			\item (+) very versatile, uses memory efficiently
			\item (-) instruction words must be decoded before number of bytes is known (harder to fetch/decode)
		\end{itemize}

		\item fixed width
		\begin{itemize}
			\item (+) every instruction word is an instruction, thus easier to fetch/decode
			\item (-) uses memory inefficiently (same num. bits even for short instructions)
		\end{itemize}

		\item hybrid
		\begin{itemize}
			\item primarily for embedded processors to conserve memory
			\item often use a subset of instructions, fewer registers, or even instruction compression
		\end{itemize}
	\end{itemize}

	\subsection{ISA Examples}

	\subsubsection{MIPS}

	Characteristics:

	\begin{itemize}
		\item load/store ISA: only loads/stores can have memory operands, makes for easy pipelining and uniform instruction width
		\item fixed instruction width = easy fetch and decode
		\item small number of addressing modes = easy pipelining
		\item large register file: 32 integer and 32 floating point registers
		\item aligned memory = easy data fetching
		\item quantitatively designed
	\end{itemize}

	\noindent Instruction format:

	\begin{itemize}
		\item R: \texttt{op, rs, rt, rd, shamt, funct} (\texttt{shamt} = shift amount, \texttt{funct} = ALU function)
		\item I: op, rs, rt, 16-bit address
		\item J: op, 26-bit address
	\end{itemize}

	\section{Memory Hierarchy}

	\subsection{Basics of Cache Architecture}

	\subsubsection{Cache Organization}

	A cache is similar to a table

	\begin{itemize}
		\item a \textbf{set} in cache = a row in table
		\item a \textbf{way} in cache = a column in table
		\item a \textbf{line} in cache = a cell in table
	\end{itemize}

	\subsubsection{Choices on Cache Associativities}

	\begin{itemize}
		\item direct mapped cache: a block can be placed in only one line in the cache (i.e. vertical array table)
		\begin{itemize}
			\item rigid placement of blocks in cache set
			\item usually has higher miss rate
			\item but power efficient (no need to search an entire way if the way is only one cell)
		\end{itemize}
		\item fully associative cache: a block can be placed in any line in the cache (i.e. a horizontal array table)
		\begin{itemize}
			\item flexible placement of blocks in cache set
			\item has the lowest miss rate
			\item but power hungry (have to search entire set [aka the whole cache] to find a block)
		\end{itemize}
		\item set-associative cache: a block can be placed in one of the ways in a set (i.e. a 2D array table)
	\end{itemize}

	\subsubsection{Cache Parameters}

	\begin{itemize}
		\item SIZE = total amount of cache data storage in bytes
		\item BLOCKSIZE = total number of bytes in a single block
		\item ASSOC = associativity (number of lines in a set)
	\end{itemize}

	\noindent Formulas:

	\# of blocks in a cache = $\frac{SIZE}{BLOCKSIZE}$ \\

	\# of sets in a cache = $\frac{\texttt{\# cache blocks}}{ASSOC} = \frac{SIZE}{BLOCKSIZE \cdot ASSOC}$ \\

	\# of index bits = $\log_2(\# sets)$ \\

	\# of block offset bits = $\log_2(BLOCKSIZE)$ \\

	\# of tag bits = 32 $-$ \texttt{\#index bits} $-$ \texttt{\#offset bits}

	\subsubsection{Examples}

	Example 1: Processor accesses a 256B direct-mapped cache, which has a block size of 32B, with the below sequence 
	of addresses. Show the contents of the cache after each access, and count the number of hits and replacements. \\

	\# index bits = $\log_2(8) = 3$

	\# offset bits = $\log_2(32) = 5$

	\# tag bits = $32 - 3 - 5 = 24$ \\

	\begin{tabular}[ht!]{|c|c|c|c|c|} \hline
		Address (hex)		& Tag (hex) 		& Index and offset bits (binary)	& Index (decimal)	& Comment	\\ \hline
		\texttt{0xFF0040E0}	& \texttt{0xFF0040}	& \textbf{111}0 0000				& 7					& miss	  	\\ \hline	
		\texttt{0xBEEF005C}	& \texttt{0XBEEF00}	& \textbf{010}1 1100				& 2					& miss		\\ \hline
		\texttt{0xFF0040E2}	& \texttt{0xFF0040}	& \textbf{111}0 0010				& 7					& hit		\\ \hline
		\texttt{0xFF0040E8}	& \texttt{0xFF0040}	& \textbf{111}0 1000				& 7					& hit		\\ \hline
		\texttt{0x00101078}	& \texttt{0x001010}	& \textbf{011}1 1000				& 3					& miss		\\ \hline
		\texttt{0x002183E0}	& \texttt{0x002183}	& \textbf{111}0 0000				& 7					& miss/rep	\\ \hline
		\texttt{0x00101064}	& \texttt{0x001010}	& \textbf{011}0 0100				& 3					& hit		\\ \hline
		\texttt{0x0012255C}	& \texttt{0x001225}	& \textbf{010}1 1100				& 2					& miss/rep	\\ \hline
		\texttt{0x00122544}	& \texttt{0x001225}	& \textbf{010}0 0100				& 2					& hit		\\ \hline
	\end{tabular}

	\subsubsection{Write Updates}

	\begin{itemize}
		\item Write-through (WT) policy
		\begin{itemize}
			\item writing to some level in the cache also means writing through to subsequent levels in the cache (i.e. 
			next level in the memory hierarchy)
		\end{itemize}

		\item Write-back (WB) policy
		\begin{itemize}
			\item write only to the specified cache level, and set its dirty bit
			\item when the block you wrote to is replaced (evicted), write the block to the next level of memory hierarchy
		\end{itemize}

		\item Write-allocate (WA) policy
		\begin{itemize}
			\item bring block into cache if the write misses (just like in read misses)
			\item typically used in conjunction with write-back (WBWA)
		\end{itemize}

		\item Write-no-allocate (NA) policy
		\begin{itemize}
			\item do not bring the block into the cache if write misses
			\item \textit{must} be used in conjunction with write-through (WTNA)
		\end{itemize}
	\end{itemize}

	\subsubsection{Victim Cache}

	\begin{itemize}
		\item small fully-associative cache that sits alongside the primary cache
		\item when main cache evicts a block, the victim cache takes the evicted block (called the ``victim'')
		\item when the main cache misses, it searches the victim cache for recently discarded blocks; a victim cache hit 
		means the main cache doesn't have to go to memory to search for a block

		\item example:
		\begin{itemize}
			\item L1 cache (initially a set contains just A)
			\item 2-entry victim cache that contains X, and Y (current LRU)
			\item B misses in L1, evicts A, A goes to victim cache and replaces Y (previous LRU); X is new LRU
			\item A then misses in L1 but hits in victim cache, so A and B \textit{swap} positions (A goes to L1, B goes to VC; 
			note that X (prev LRU) is not replaced in the case of victim cache hits)
			\item thus a victim cache is useful in cases of repeated conflicts and gives the illusion of set-associativity
		\end{itemize}
	\end{itemize}

	\subsection{Replacement Policies}

	\subsubsection{Optimal Replacement Policy}

	\begin{itemize}
		\item look into future and determine when each block in a set is needed again (if at all)
		\item replace the block needed farthest in the future
		\item note: not practical since we don't know in advance when blocks are needed, but this is the theoretical gold 
		standard with which other replacement policies are evaluated against
	\end{itemize}

	\subsubsection{LRU implementation}

	\begin{itemize}
		\item assign a row and column in LRU matrix to each way in the set
		\item if there's a hit in a way, set the row corresponding to that way, and unset the column corresponding to that way
		\item the number of 1's in a row specifies the MRU order (thus, the LRU is the one with all 0's)
	\end{itemize}

	\begin{figure}[ht!]
		\centering

		\includegraphics[width=\textwidth]{assets/lru-matrix-implementation.png}
	\end{figure}

	\subsubsection{Pseudo-LRU implementation}

	\begin{itemize}
		\item LRU implementation takes $O(way^2)$ space and time; too expensive
		\item PLRU approximates LRU with decent accuracy
		\item PLRU complexity is $O(way)$
	\end{itemize}

	\begin{figure}[ht!]
		\centering
		\includegraphics[width=\textwidth]{assets/plru-initial-state.png}
	\end{figure}

	\begin{figure}[ht!]
		\centering
		\includegraphics[scale=0.7]{assets/plru-walkthrough.png}
	\end{figure}

	\newpage

	\subsection{Inclusion Policies}

	\begin{itemize}
		\item block in inner cache level \textit{always} included in outer level cache too: \textbf{inclusive}
		\item block in inner cache level \textit{never} included in outer level cache too: \textbf{exclusive}
		\item block in inner cache level \textit{sometimes} included in outer level cache: \textbf{non-inclusive}
	\end{itemize}

	\begin{center}
		\includegraphics[scale=0.5]{assets/inclusion-policy-ex1.png}
		\includegraphics[scale=0.5]{assets/inclusion-policy-ex2.png}
		\includegraphics[scale=0.5]{assets/inclusion-policy-ex3.png}
		\includegraphics[scale=0.5]{assets/inclusion-policy-ex4.png}
		\includegraphics[scale=0.5]{assets/inclusion-policy-ex5.png}
		\includegraphics[scale=0.5]{assets/inclusion-policy-ex6.png}
	\end{center}

	\subsubsection{Inclusive outer cache Pros/Cons}

	Pros:

	\begin{itemize}
		\item for most cases, external requests can be checked against the outer cache (if not in outer, cannot be in inner)
		\item reduces contention for cache tags at inner cache
		\item external snoop latency (and thus memory latency) is reduced significantly
	\end{itemize}
	
	\noindent Cons:

	\begin{itemize}
		\item space wasteful; forced data redundancy
		\item inflexible (power gating ways makes inner cache ineffective)
	\end{itemize}

\end{document}
