\documentclass[12pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage[legalpaper, portrait, margin=0.8in]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}

\title{CDA5106 - Advanced Computer Architecture \\ Final Exam Review}
\author{Kobee Raveendran}
\date{}

\begin{document}
    \maketitle

    \section{Module 1: High-Performance Microprocessor Architecture}

    \subsection{Module 1.2: Power Wall and Dennard Scaling}

    \subsubsection{Notes}

    \begin{itemize}
        \item energy: ability of a physical system to do work on other physical systems (unit: joule)
        \item power: rate at which energy is transformed (unit: watt; 1 watt = 1 joule delivered per second)
        
        \begin{itemize}
            \item power = $V \cdot I$ (V = voltage, I = current)
        \end{itemize}
        
        \item for capacitors:
        
        \begin{itemize}
            \item energy stored = $0.5 \cdot C \cdot V^2$ (C = capacitance, V = voltage)
            \item if a capacitor is drained at a frequency of $f$ per second: 
            power = $\frac{energy}{second} = 2 \cdot 0.5 CV^2 = CV^2$
        \end{itemize}

        \item Power wall problem
        \begin{itemize}
            \item $P_{dyn} = ACV^2f$
            \item A: fraction of gates actively switching
            \item C: total capacitance of all gates
            \item V: supply voltage
            \item f: frequency of switching
        \end{itemize}

        \item Power wall fundamentals
        \begin{itemize}
            \item max frequency vs. threshold voltage:
            \item $f_{max} = c \cdot \frac{(V - V_{thd})^{1.3}}{V}$
        \end{itemize}

        \item \textbf{Dennard Scaling Example (old)}
        \begin{itemize}
            \item if gate length (transistor size) scales by $S = 0.7$ (both length and width), then:
            \item capacitance scales by $S = 0.7$
            \item original area scales by $S^2 = 0.5$
            \item number of transistors scales by $\frac{1}{S^2} \approx 2$
            \item supply voltage ($V$) scales by $S = 0.7$
            \item frequency ($f$) scales by $\frac{1}{S} = 1.4$
            \item then, \textbf{dynamic power} $P_{dyn} = ACV^2f$
            \item and \textbf{new dynamic power} $P_{dyn}' = A'C'V'^2f'$
            \item $P_{dyn}' = (2A)(0.7C)(0.7V)^2(1.4f) \approx 1 \cdot ACV^2f = P_{dyn}$
        \end{itemize}

        \item \textbf{Post Dennard Scaling example (new)}
        \begin{itemize}
            \item capacitance scales by $S = 0.7$
            \item number of transistors scales by $\frac{1}{S^2} = 2$
            \item supply voltage ($V$) cannot scale without also scaling threshold voltage ($V_{thd}$), and doing that 
            increases static power exponentially
            \item frequency ($f$) scales by $\frac{1}{S} = 1.4$
            \item result: dynamic power doubles every generation
            \item $P_{dyn} = ACV^2f$
            \item $P_{dyn}' = A'C'V'^2f' = (2A)(0.7C)(1 \cdot V)^2(1.4f) \approx 2 \cdot P_{dyn}$
        \end{itemize}

        \subsubsection{Exercises}

			\begin{enumerate}
				\item Suppose that instead of progressing at a ratio of 0.7, Mooreâ€™s law slows down and transistor gate length scales at a ratio of 0.8 instead. Find the dynamic power consumption under \textit{unlimited} and \textit{limited} scaling for the next process generation.

				\begin{itemize}
					\item Unlimited/old scaling rule
					\begin{itemize}
						\item gate length scales by $S = 0.8$
						\item capacitance scales by $S = 0.8$
						\item original area scales by $S^2 = 0.64$
						\item num transistors thus scales by $\frac{1}{S^2} = 1.56$
						\item supply voltage scales by $S = 0.8$
						\item frequency scales by $\frac{1}{S} = 1.25$
						\item dynamic power stays constant:
					
						$P_{dyn} = (1.56A)(0.8C)(0.8V)^2(1.25f)$
					\end{itemize}
					
					\item leakage-limited/new scaling
					\begin{itemize}
						\item capacitance scales by $S = 0.8$
						\item num transistors scales by $\frac{1}{S^2} = 1.56$
						\item supply voltage does not scale without scaling threshold voltage too, which increases static power exponentially
						\item frequency scales by $\frac{1}{S} 1.25$
						\item dynamic power consumption increases:

						$P_{dyn}' = (1.56A)(0.8C)(V)^2(1.25f) = 1.56 \cdot P_{dyn}$
					\end{itemize}
				\end{itemize}

				\item With limited voltage scaling, suppose that we want to keep the dynamic power consumption constant in the next generation by keeping frequency constant and reduce die area. How much should we reduce die area to achieve that?
				\begin{itemize}
					\item gate length scales by $S = 0.7$
					\item capacitance scales by $S = 0.7$
					\item original area scales by $S^2 = 0.5$
					\item supply voltage and frequency are constant
					\item dynamic power consumption must stay constant: $P_{dyn}' = P_{dyn}$

					$ACV^2f = A'(0.7C)V^2f \longrightarrow A = 0.7A'$

					\item number of transistors in the next generation: $A' = 1.4A$ (instead of 2A like before; i.e. 70\% of 2A)
					\item thus die area shrinks by 30\%
				\end{itemize}

				\item Describe the difference between energy and power.

				Power is the rate of energy consumption.

				\item Describe the impact of threshold voltage choice on static and dynamic power consumption as transistors are scaled down.

				If threshold voltage is lowered, dynamic power decreases (nearly linearly) but static power increases exponentially.

				\item How has processor design adapted to the power wall problem?

				Stalling frequency growth, multicore, and sophisticated power management (clock gating, voltage and frequency scaling, power gating).
			\end{enumerate}

    \end{itemize}

	\subsubsection{Overview of ILP Techniques}

	Caches example

	\begin{itemize}
		\item processor with 1-ns clock
		\item 64KB cache memory with 2-ns read time, 95\% hitrate
		\item 512MB main memory with 150-ns read time
		\item What is the average access time (AAT) in this memory system?
	\end{itemize}

	Answer:
	\begin{itemize}
		\item hits: $95 \cdot 2$ ns, misses: $5 \cdot (2 + 150)$ ns
		\item total = hit time + miss time = $190 + (10 + 750) = 950 ns$
		\item AAT = $\frac{total}{100} = 9.5ns$
	\end{itemize}

	\section{Module 2: Performance, Cost, and Reliability of Microprocessors}

	\subsection{Performance Evaluation 1}

	\subsubsection{Amdahl's Law}

	\begin{itemize}
		\item performance improvement (``speedup'') is limited by the part you can't improve
		\item (s) $Speedup_{enhanced} =$ best case speedup from gizmo alone
		\item (f) $Fraction_{enhanced} =$ fraction of task that gizmo can enhance
		\item $s_{overall} = \frac{1}{(1 - f) + \frac{f}{s}}$
	\end{itemize}

	Example:

	\begin{itemize}
		\item jet plane wing simulation, where 1 run takes 1 week on your computer
		\item your program is 80\% parallelizable
		\item new supercomputer has 100,000 processors
		\item $s = 100,000$
		\item $f = 0.8$
		\item overall speedup: $s_{overall} = \frac{1}{(1 - f) + \frac{f}{s}} = \frac{1}{(1 - 0.8) + \frac{0.8}{100000}} \approx \frac{1}{0.2} = 5$
		\item only about 5 times faster (33 hours instead of 1 week), but not worth the high price tag (using a cheaper computer with only 100 processors instead yields a 4.8X speedup!)
	\end{itemize}

	More examples:

	Ex 1:

	\begin{itemize}
		\item $f = 0.95$
		\item $s = 1.10$
		\item $s_{overall} = \frac{1}{(1-0.95) + \frac{0.95}{1.10}} = 1.094 \approx 1.10$
	\end{itemize}

	Ex 2:

	\begin{itemize}
		\item $f = 0.05$
		\item $s \rightarrow \infty$
		\item $s_{overall} = 1.053$
	\end{itemize}

	\subsubsection{Run Time}

	\begin{itemize}
		\item CPU time = clock cycle count $\times$ cycle time
		\item cycles per instruction (CPI) = $\frac{\texttt{clock cycle count}}{\texttt{instruction count}}$
		\item CPU time = IC $\times$ CPI $\times$ CT
	\end{itemize}

	\subsection{Performance Evaluation 2}

	Determine speedup by comparing program times with respect to a reference machine.

	

	\begin{itemize}
		\item arithmetic mean (which one should we trust?):
		
		\begin{tabular}[h!]{|c|c|c|c|} \hline
					   & Computer A & Computer B & B vs. A   \\ \hline
			Program P1 & 2X faster  & 4X faster  & 2X faster \\ \hline
			Program P2 & 5X faster  & 15X faster & 3X faster \\ \hline
			Average	   & 3.5X		& 9.5X		 &			 \\ \hline
		\end{tabular}

		Speedups:

		\begin{itemize}
			\item method 1: program-wise $\longrightarrow \frac{2 + 3}{2} = 2.5$X faster
			\item method 2: machine-wise $\longrightarrow \frac{9.5}{3.5} = 2.71$X faster
		\end{itemize}

		\item geometric mean (consistent):
		
		$\displaystyle gmean = \sqrt[n]{\prod_{i = 1}^n} = exp(\frac{\frac{1}{n} \sum_{i=1}^n \ln (x_i)}{n})$

		\begin{tabular}[h!]{|c|c|c|c|} \hline
					   & Computer A 	& Computer B 	& B vs. A   	\\ \hline
			Program P1 & 2X faster  	& 4X faster  	& 2X faster 	\\ \hline
			Program P2 & 5X faster  	& 15X faster 	& 3X faster 	\\ \hline
			Average	   & $\sqrt{10}$	& $\sqrt{60}$	& $\sqrt{6}$	\\ \hline
		\end{tabular}

		Speedups:

		\begin{itemize}
			\item method 1: program-wise $\longrightarrow$ B is $\sqrt{2 \cdot 3} = \sqrt{6}$X faster
			\item method 2: machine-wise $\longrightarrow$ B is $\sqrt{60} \cdot \sqrt{10} = \sqrt{6}$X faster
		\end{itemize}

		\item (also important): geometric standard deviation
		
		$\displaystyle gstdev = exp(\sqrt{\frac{\prod_{i=1}^n (\ln x_i - \ln gmean)^2}{n}})$

		in plain English: for each ``component'' speedup vs. ref machine, take its natural log and subtract the natural 
		log of the gmean from that. Square it and multiply all of these together, then divide by n. Finally take the 
		square root of this, then take $e$ to the power of the result.
	\end{itemize}

	\subsubsection{Exercises}

	Given the following table of speedups for machines A and B relative to a reference machine:

	\begin{tabular}[h!]{|c|c|c|c|} \hline
		Prog	& X (secs)	& A (secs)	& B	(secs)	\\ \hline
		App 1	& 30		& 15		& 10		\\ \hline
		App 2	& 20		& 15		& 10		\\ \hline
		App 3	& 40		& 20		& 30		\\ \hline
		App 4	& 15		& 20		& 15		\\ \hline
	\end{tabular}

	Compute the following (see post-computation table below to find them all):

	\begin{itemize}
		\item geometric speedup of machine A vs. base machine X
		
		from the table, we find that A has a 1.41X speedup over X

		\item geometric speedup of machine B vs. base machine X
		
		from the table, we find that B has a 1.68X speedup over X

		\item geometric speedup of machine B vs. machine A
		
		from the table, we find that B has a 1.19X speedup over A

		\item geometric standard deviation of the speedup of machine A over machine X
		
		$gstd = exp(\sqrt{\frac{1}{4} \cdot \ln^2(\frac{2}{1.41}) \ln^2(\frac{1.33}{1.41}) \ln^2(\frac{2}{1.41}) \ln^2(\frac{0.75}{1.41})})$

		$gstd = 1.002255... \approx 1$

	\end{itemize}

	\begin{tabular}[h!]{|c|c|c|c|} \hline
		Prog		& A vs. X	& B vs. X	& B vs. A	\\ \hline
		App 1		& 2X		& 3X		& 1.5X		\\ \hline
		App 2		& 1.33X		& 2X		& 1.5X		\\ \hline
		App 3		& 2X		& 1.33X		& 0.67X		\\ \hline
		App 4		& 0.75X		& 1X		& 1.33X		\\ \hline
		\bf{Product}& 4X		& 8X		& 2X		\\ \hline
		\bf{gmean}	& 1.41X		& 1.68X		& \bf{1.19X}\\ \hline
	\end{tabular}

	\subsection{Cost and Reliability}

	\subsubsection{Failure Rates ($\lambda$)}

	\begin{itemize}
		\item $\lambda$ = the number of failures that occur per unit time in a component/system
		\item FIT (failure in time) = number of failures in $10^9$ hours
		\item example: 10,000 microprocessor chips used for 1,000 hours, and 8 of them fail. Failure rate is thus 
		$\frac{8}{10,000 \cdot 1,000} = 8 \cdot 10^{-7}$ (failures per hour per chip) $\cdot 10^9$ hours = 800 FITs
	\end{itemize}

	\subsubsection{Reliability Metrics}

	\begin{itemize}
		\item $R(t)$ = probability that the system still works correctly at time $t$
		\item $W_N(t)$ = number of items (of the same kind) that would still be working at time $t$
		\item if $\lambda$ is constant, then $R(t) = e^{-\lambda t}$
		\item Mean Time Between Failure (MTBF) = $\frac{1}{\lambda}$
	\end{itemize}

	\subsubsection{System Reliability}

	Assume that:

	\begin{itemize}
		\item $M$ components are in the system with failure rates $\lambda_1, \lambda_2, ..., \lambda_m$
		\item for the system to work properly, all components must also work properly
		\item a component's reliability is independent of any other component's reliability
		\item then, system failure rate = sum of component's failure rates
		\item $R_{sys}(t) = R_1(t) \cdot R_2(t) \cdot ... \cdot R_m(t) = e^{-\lambda_1 t \cdot ... \cdot -\lambda_mt}$
		$= e^{-(\lambda_1 + \lambda_2 + ... + \lambda_m)t} = e^{-\lambda_{sys}t}$
	\end{itemize}

	Other metrics:

	\begin{itemize}
		\item Mean Time To Repair (MTTR): mean time to repair/recover from a fault
		\item Mean Time Between Failure (MTBF): mean time between 2 consecutive failures
		\item if each failure is repaired, then MTBF = MTTF + MTTR
		\item usually, MTTF $\gg$ MTTR, so MTBF and MTTF are often interchangeable
	\end{itemize}

	\subsubsection{Examples}

	Assume a disk subsystem with:

	\begin{itemize}
		\item 10 disks each rated at $10^6$-hour MTTF
		\item 1 SCSI controller rated at $5 \cdot 10^5$-hour MTTF
		\item 1 power supply rated at $2 \cdot 10^5$-hour MTTF
		\item 1 fan rated at $2 \cdot 10^5$-hour MTTF
		\item 1 SCSI cable rated at $10^6$-hour MTTF
	\end{itemize}

	Find the failure rate of the entire disk subsystem.

	$R_{sys}(t) = 10 \cdot \frac{1}{10^6} + \frac{1}{5 \cdot 10^5} + \frac{2}{2 \cdot 10^5} + \frac{1}{10^6}$
	$			= \frac{10 + 2 + 5 + 5 + 1}{10^6} = \frac{23}{10^6} = \frac{23,000}{10^9} = 23,000$ FIT

	Thus, MTTF = $\frac{1}{\lambda_{sys}} = \frac{1}{23,000} \cdot 10^9 \approx 43,500$ hours

	\section{Instruction Set Design}

	\subsection{Instruction Set Architecture 1}

	\subsubsection{Styles of ISAs}

	\begin{itemize}
		\item Stack:
		\begin{itemize}
			\item \texttt{push <addr>, pop <addr>} (or ALU instructions)
			\item ALU: pop two entries, perform ALU operation, push result onto stack
			\item compact instruction format
			\begin{itemize}
				\item all calculation operations take 0 operands
				\item flexible; used for compiling Java bytecode (not dependent on registers in architecture [but all 
				have stacks])
			\end{itemize}
		\end{itemize}

		\item Accumulator:
		\begin{itemize}
			\item \texttt{load/store/ALU <addr>} (result affects accumulator register)
			\item also very compact (all operations take 1 operand, the other is implicitly the accumulator register)
			\item less dependence on memory than stack-based
		\end{itemize}

		\item Register-memory:
		\begin{itemize}
			\item \texttt{load/store/ALU <reg>, <reg/addr>}
			\begin{itemize}
				\item at most one operand can be a memory address
				\item leftmost register is the destination (if applicable)
			\end{itemize}
		\end{itemize}

		\item Load-Store:
		\begin{itemize}
			\item \texttt{load/store <reg>, <addr>}
			\item ex: \texttt{ALU <reg1>, <reg2>, <reg3>}: reg1 is destination, other 2 are source registers
		\end{itemize}
	\end{itemize}

	Example for adding two numbers:

	\begin{tabular}[ht!]{|c|c|c|c|} \hline
		Stack	& Accumulator	& Register-memory	& Load-Store	\\ \hline
		push A	& load A		& load R1 A			& load R1 A		\\ \hline
		push B	& add B			& add R1 B			& load R2 B		\\ \hline
		add		& store C		& store C R1		& add R3 R1 R2	\\ \hline
		pop C	& 				& 					& store C R3	\\ \hline
	\end{tabular}

	Pros/cons:

	\begin{itemize}
		\item load-store
		\begin{itemize}
			\item (+) fixed length instructions possible (allows for easy fetch/decode)
			\item (+) simpler hardware: efficient pipeline and potentially lower cycle time
			\item (-) higher instruction count
			\item (-) fixed-length instructions can be wasteful (more bits than needed for some instructions)
		\end{itemize}

		\item register-memory
		\begin{itemize}
			\item (+) no need for extra loads
			\item (+) better usage of bits (these pros lead to better code density)
			\item (-) destroys source operand(s) (i.e. \texttt{add R1 R2})
			\item (-) may impact cycles per instruction
		\end{itemize}

		\item memory-memory
		\begin{itemize}
			\item (+) most compact (code density)
			\item (-) high memory traffic (thus bottlenecked by memory)
		\end{itemize}
	\end{itemize}

	\subsection{Instruction Set Architecture 2}

	\subsubsection{RISC and Common Addressing Modes}

	\begin{itemize}
		\item register
		\begin{itemize}
			\item \texttt{add R4 R3 // R4 = R4 + R3}
			\item used when value is in a register
		\end{itemize}

		\item immediate
		\begin{itemize}
			\item \texttt{add R4 \#3 // R4 = R4 + 3}
			\item used for small constants (which occur frequently)
		\end{itemize}

		\item displacement
		\begin{itemize}
			\item \texttt{add R4 100(R1) // R4 = R4 + MEM[100 + R1]}
			\item accesses the frame (arguments, local variables)
			\item access the global data segment
			\item accesses the fields of a data struct
		\end{itemize}

		\item register deferred/register indirect
		\begin{itemize}
			\item \texttt{add R3 (R1) // R3 = R3 + MEM[R1]}
			\item accesses using a computed memory address
		\end{itemize}

		\item indexed
		\begin{itemize}
			\item \texttt{add R3 (R1 + R2) // R3 = R3 + MEM[R1 + R2]}
			\item array accesses: R1 = base, R2 = index
		\end{itemize}

		\item direct/absolute
		\begin{itemize}
			\item \texttt{add R1 (1001) // R1 = R1 + MEM[1001]}
			\item accessing global (``static'') data
		\end{itemize}

		\item memory deferred/memory indirect
		\begin{itemize}
			\item \texttt{add R1 @(R3) // R1 = R1 + MEM[MEM[R3]]}
			\item pointer dereferencing: \texttt{x = *p;} (if p is not register-allocated)
		\end{itemize}

		\item autoincrement/postdecrement
		\begin{itemize}
			\item \texttt{add R1 (R2)+ // R1 = R1 + MEM[R2]; R2 = R2 + \textit{d}} (\textit{d} is the size of the operation)
			\item looping through arrays, stack pop
		\end{itemize}

		\item autodecrement/predecrement
		\begin{itemize}
			\item \texttt{add R1 -(R2) // R1 = R1 + MEM[R2]; R2 = R2 - \textit{d}} (\textit{d} is the size of the operation)
			\item same uses as autoincrement, stack push
		\end{itemize}

		\item scaled
		\begin{itemize}
			\item \texttt{add R1 100(R2)[R3] // R1 = R1 + MEM[100 + R2 + R3 * \textit{d}]}
			\item array accesses for non-byte-sized elements
		\end{itemize}
	\end{itemize}

	\subsection{Instruction Set Architecture 3}

	\subsubsection{Condition Codes (for branch instructions)}

	\begin{itemize}
		\item Z
		\begin{itemize}
			\item zero flag
			\item indicates the result of an arithmetic/logical expression is zero
		\end{itemize}

		\item C
		\begin{itemize}
			\item carry flag
			\item indicates that an operation has a carry out. Enables numbers larger than a single word to be added/subtracted
		\end{itemize}

		\item S/N
		\begin{itemize}
			\item sign/negative flag
			\item indicates the result of an operation is negative
		\end{itemize}

		\item V/O/W
		\begin{itemize}
			\item overflow flag
			\item indicates the result of an operation is too large to fit in a register (using 2's complement representation)
		\end{itemize}
	\end{itemize}

	\subsubsection{Instruction Encoding Tradeoffs}

	\begin{itemize}
		\item variable width
		\begin{itemize}
			\item (+) very versatile, uses memory efficiently
			\item (-) instruction words must be decoded before number of bytes is known (harder to fetch/decode)
		\end{itemize}

		\item fixed width
		\begin{itemize}
			\item (+) every instruction word is an instruction, thus easier to fetch/decode
			\item (-) uses memory inefficiently (same num. bits even for short instructions)
		\end{itemize}

		\item hybrid
		\begin{itemize}
			\item primarily for embedded processors to conserve memory
			\item often use a subset of instructions, fewer registers, or even instruction compression
		\end{itemize}
	\end{itemize}

	\subsection{ISA Examples}

	\subsubsection{MIPS}

	Characteristics:

	\begin{itemize}
		\item load/store ISA: only loads/stores can have memory operands, makes for easy pipelining and uniform instruction width
		\item fixed instruction width = easy fetch and decode
		\item small number of addressing modes = easy pipelining
		\item large register file: 32 integer and 32 floating point registers
		\item aligned memory = easy data fetching
		\item quantitatively designed
	\end{itemize}

	\noindent Instruction format:

	\begin{itemize}
		\item R: \texttt{op, rs, rt, rd, shamt, funct} (\texttt{shamt} = shift amount, \texttt{funct} = ALU function)
		\item I: op, rs, rt, 16-bit address
		\item J: op, 26-bit address
	\end{itemize}

\end{document}
